
Visual and analytic solution comparison
=======================================

The idea here is to compare the various analytic functions with
approximate solutions, to verify that the analytic implementation is
correct. Ideally, we want to see two things:

1. As the number of approximation points increases, the error should go
   down and tend towards zero.

2. With a sufficient number of approximation points, the analytic and
   approximate solutions should appear visually identical.

.. code:: python

    # imports 
    %matplotlib inline
    from bayesian_quadrature import BQ, bq_c, util
    
    # set loglevel to debug
    import logging
    logger = logging.getLogger("bayesian_quadrature")
    logger.setLevel("DEBUG")
.. code:: python

    # seed the numpy random generator, so we always get the same randomness
    np.random.seed(8728)
.. code:: python

    # parameters
    options = {
        'ntry': 10,
        'n_candidate': 10,
        'x_mean': 0.0,
        'x_var': 10.0,
        'candidate_thresh': 0.5,
    }
    
    # different number of approximation points to try, to see if the
    # approximation converges, and the range over which we will compute
    # the approximation points
    N = np.logspace(np.log10(30), np.log10(500), 100)
    xmin, xmax = -10, 10
Here we define some helper functions to plot the error/visual
comparisons, and to print information about the worst error and
approximate/analytic solution values.

.. code:: python

    def plot_0d(calc, approx):
        fig, ax = plt.subplots()
        ax.plot(N, np.abs(calc-approx), lw=2, color='r')
        ax.set_title("Error")
        ax.set_xlabel("# approximation points")
        ax.set_xlim(N.min(), N.max())
        util.set_scientific(ax, -5, 4)
.. code:: python

    def print_0d(calc, approx):
        err = np.abs(calc - approx[-1])
        print "error : %s" % err
        print "approx: %s" % approx[-1]
        print "calc  : %s" % calc
.. code:: python

    def plot_1d(calc, approx):
        fig, (ax1, ax2) = plt.subplots(1, 2)
    
        ax1.plot(N, np.abs(calc-approx).sum(axis=-1), lw=2, color='r')
        ax1.set_title("Error")
        ax1.set_xlabel("# approximation points")
        ax1.set_xlim(N.min(), N.max())
        util.set_scientific(ax1, -5, 4)
    
        ax2.plot(approx[-1], lw=2, label="approx")
        ax2.plot(calc, '--', lw=2, label="calc")
        ax2.set_title("Comparison")
        ax2.legend(loc=0)
        util.set_scientific(ax2, -5, 4)
    
        fig.set_figwidth(8)
        plt.tight_layout()
.. code:: python

    def print_1d(calc, approx):
        maxerr = np.abs(calc - approx[-1]).max()
        approx_sum, calc_sum = approx[-1].sum(), calc.sum()
        print "max error  : %s" % maxerr
        print "sum(approx): %s" % approx_sum
        print "sum(calc)  : %s" % calc_sum
.. code:: python

    def plot_2d(calc, approx):
        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)
    
        ax1.plot(N, np.abs(calc-approx).sum(axis=-1).sum(axis=-1), lw=2, color='r')
        ax1.set_title("Error")
        ax1.set_xlabel("# approximation points")
        ax1.set_xlim(N.min(), N.max())
        util.set_scientific(ax1, -5, 4)
    
        vmax = max(approx[-1].max(), calc.max())
    
        ax2.imshow(approx[-1], cmap='gray', interpolation='nearest', vmin=0, vmax=vmax)
        ax2.set_title("Approximation")
    
        ax3.imshow(calc, cmap='gray', interpolation='nearest', vmin=0, vmax=vmax)
        ax3.set_title("Calculated")
    
        i = 0
        ax4.plot(approx[-1, i], lw=2, label='approx')
        ax4.plot(calc[i], '--', lw=2, label='calc')
        ax4.set_title("Comparison at index %d" % i)
        ax4.legend(loc=0)
        util.set_scientific(ax4, -5, 4)
    
        fig.set_figwidth(14)
        plt.tight_layout()
Now we create the Bayesian Quadrature object that we'll be testing
against.

.. code:: python

    # choose points and evaluate them under a standard normal distribution
    x = np.linspace(-5, 5, 9)
    f_y = lambda x: scipy.stats.norm.pdf(x, 0, 1)
    y = f_y(x)
    
    # create the bayesian quadrature object
    bq = BQ(x, y, **options)
    bq.fit_log_l((30, 5, 0))
    bq.fit_l((y.max(), 1, 0))
    
    # plot the BQ approximation
    fig, axes = bq.plot(xmin=xmin, xmax=xmax)

.. parsed-literal::

    DEBUG:bayesian_quadrature:Setting parameters for GP over log(l)
    DEBUG:bayesian_quadrature:Kxx conditioning number is 2993734.68101
    DEBUG:bayesian_quadrature:Choosing candidate points
    DEBUG:bayesian_quadrature:Setting parameters for GP over exp(log(l))
    DEBUG:bayesian_quadrature:Kxx conditioning number is 18.7987497995
    DEBUG:bayesian_quadrature:Adding jitter to indices [ 7  9 10 11 12]
    DEBUG:bayesian_quadrature:Kxx conditioning number is now 18.7860902033



.. image:: visual-tests_files/visual-tests_12_1.png


.. code:: python

    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)
    xmin, xmax = -10, 10
    bq.plot_l(ax1, f_l=f_y, xmin=xmin, xmax=xmax)
    bq.plot_expected_squared_mean(ax2, xmin=xmin, xmax=xmax)
    bq.plot_expected_variance(ax3, xmin=xmin, xmax=xmax)
    fig.set_figheight(8)
    plt.tight_layout()


.. image:: visual-tests_files/visual-tests_13_0.png


int\_K
------

Test
:math:`\int K(x^\prime, x) \mathcal{N}(x^\prime | \mu, \Sigma)\ \mathrm{d}x^\prime`

.. code:: python

    calc = np.empty(bq.nsc)
    bq_c.int_K(
        calc, bq.gp_l.x[:, None],
        bq.gp_l.K.h, np.array([bq.gp_l.K.w]),
        bq.x_mean, bq.x_cov)
    
    approx = np.empty((N.size, bq.nsc))
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq_c.approx_int_K(xo, bq.gp_l, bq.x_mean, bq.x_cov)
.. code:: python

    plot_1d(calc, approx)
    print_1d(calc, approx)

.. parsed-literal::

    max error  : 2.56704190814e-06
    sum(approx): 0.125943330984
    sum(calc)  : 0.125946448156



.. image:: visual-tests_files/visual-tests_16_1.png


int\_K1\_K2
-----------

Test
:math:`\int K_1(x_1, x^\prime) K_2(x^\prime, x_2) \mathcal{N}(x^\prime | \mu, \Sigma)\ \mathrm{d}x^\prime`

.. code:: python

    calc = np.empty((bq.nsc, bq.ns))
    bq_c.int_K1_K2(
        calc, bq.gp_l.x[:, None], bq.gp_log_l.x[:, None],
        bq.gp_l.K.h, np.array([bq.gp_l.K.w]),
        bq.gp_log_l.K.h, np.array([bq.gp_log_l.K.w]),
        bq.x_mean, bq.x_cov)
    
    approx = np.empty((N.size, bq.nsc, bq.ns))
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq_c.approx_int_K1_K2(
            xo, bq.gp_l, bq.gp_log_l, bq.x_mean, bq.x_cov)
.. code:: python

    plot_2d(calc, approx)
    print_1d(calc, approx)

.. parsed-literal::

    max error  : 0.000105501273301
    sum(approx): 60.2728482429
    sum(calc)  : 60.2732411992



.. image:: visual-tests_files/visual-tests_19_1.png


int\_int\_K1\_K2\_K1
--------------------

Test
:math:`\int \int K_1(x, x_1^\prime) K_2(x_1^\prime, x_2^\prime) K_1(x_2^\prime, x) \mathcal{N}(x_1^\prime | \mu, \Sigma) \mathcal{N}(x_2^\prime | \mu, \Sigma)\ \mathrm{d}x_1^\prime\ \mathrm{d}x_2^\prime`

.. code:: python

    calc = np.empty((bq.nsc, bq.nsc))
    bq_c.int_int_K1_K2_K1(
        calc, bq.gp_l.x[:, None],
        bq.gp_l.K.h, np.array([bq.gp_l.K.w]),
        bq.gp_log_l.K.h, np.array([bq.gp_log_l.K.w]),
        bq.x_mean, bq.x_cov)
        
    approx = np.empty((N.size, bq.nsc, bq.nsc))
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq_c.approx_int_int_K1_K2_K1(
            xo, bq.gp_l, bq.gp_log_l, bq.x_mean, bq.x_cov)
.. code:: python

    plot_2d(calc, approx)
    print_1d(calc, approx)

.. parsed-literal::

    max error  : 7.5782619634e-07
    sum(approx): 0.864015199079
    sum(calc)  : 0.864026031844



.. image:: visual-tests_files/visual-tests_22_1.png


int\_int\_K1\_K2
----------------

Test
:math:`\int \int K_1(x_2^\prime, x_1^\prime) K_2(x_1^\prime, x) \mathcal{N}(x_1^\prime | \mu, \Sigma) \mathcal{N}(x_2^\prime | \mu, \Sigma)\ \mathrm{d}x_1^\prime\ \mathrm{d}x_2^\prime`

.. code:: python

    calc = np.empty(bq.ns)
    bq_c.int_int_K1_K2(
        calc, bq.gp_log_l.x[:, None],
        bq.gp_l.K.h, np.array([bq.gp_l.K.w]),
        bq.gp_log_l.K.h, np.array([bq.gp_log_l.K.w]),
        bq.x_mean, bq.x_cov)
    
    approx = np.empty((N.size, bq.ns))
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq_c.approx_int_int_K1_K2(
            xo, bq.gp_l, bq.gp_log_l, bq.x_mean, bq.x_cov)
.. code:: python

    plot_1d(calc, approx)
    print_1d(calc, approx)

.. parsed-literal::

    max error  : 6.62100940785e-06
    sum(approx): 6.93025978949
    sum(calc)  : 6.93030166067



.. image:: visual-tests_files/visual-tests_25_1.png


int\_int\_K
-----------

Test
:math:`\int \int K(x_1^\prime, x_2^\prime) \mathcal{N}(x_1^\prime | \mu, \Sigma) \mathcal{N}(x_2^\prime | \mu, \Sigma)\ \mathrm{d}x_1^\prime\ \mathrm{d}x_2^\prime`

.. code:: python

    calc = bq_c.int_int_K(
        1, bq.gp_l.K.h, np.array([bq.gp_l.K.w]),
        bq.x_mean, bq.x_cov)
    
    approx = np.empty(N.size)
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq_c.approx_int_int_K(xo, bq.gp_l, bq.x_mean, bq.x_cov)
.. code:: python

    plot_0d(calc, approx)
    print_0d(calc, approx)

.. parsed-literal::

    error : 2.90604660867e-07
    approx: 0.0138551566723
    calc  : 0.013855447277



.. image:: visual-tests_files/visual-tests_28_1.png


int\_K1\_dK2
------------

Test
:math:`\int K_1(x_1, x^\prime) \frac{\partial K_2}{\partial w_2}(x\prime, x_2) \mathcal{N}(x^\prime | \mu, \Sigma)\ \mathrm{d}x^\prime`

.. code:: python

    calc = np.empty((bq.nsc, bq.ns, 1))
    bq_c.int_K1_dK2(
        calc, bq.gp_l.x[:, None], bq.gp_log_l.x[:, None],
        bq.gp_l.K.h, np.array([bq.gp_l.K.w]),
        bq.gp_log_l.K.h, np.array([bq.gp_log_l.K.w]),
        bq.x_mean, bq.x_cov)
    
    approx = np.empty((N.size, bq.nsc, bq.ns, 1))
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq_c.approx_int_K1_dK2(
            xo, bq.gp_l, bq.gp_log_l, bq.x_mean, bq.x_cov)
.. code:: python

    plot_2d(calc[..., 0], approx[..., 0])
    print_1d(calc[..., 0], approx[..., 0])

.. parsed-literal::

    max error  : 0.0285703576777
    sum(approx): -6.41184043406
    sum(calc)  : -6.4761234464



.. image:: visual-tests_files/visual-tests_31_1.png


int\_dK
-------

Test
:math:`\int \frac{\partial K}{\partial w}(x^\prime, x) \mathcal{N}(x^\prime | \mu, \sigma)\ \mathrm{d}x^\prime`

.. code:: python

    calc = np.empty((bq.nsc, 1))
    bq_c.int_dK(
        calc, bq.gp_l.x[:, None],
        bq.gp_l.K.h, np.array([bq.gp_l.K.w]),
        bq.x_mean, bq.x_cov)
    
    approx = np.empty((N.size, bq.nsc, 1))
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq_c.approx_int_dK(
            xo, bq.gp_l, bq.x_mean, bq.x_cov)
.. code:: python

    plot_1d(calc[..., 0], approx[..., 0])
    print_1d(calc[..., 0], approx[..., 0])

.. parsed-literal::

    max error  : 1.023203096e-05
    sum(approx): -0.00119125374514
    sum(calc)  : -0.00117714484755



.. image:: visual-tests_files/visual-tests_34_1.png


Mean of Z
---------

Test :math:`E[Z]`.

.. code:: python

    calc = bq._exact_Z_mean()
    approx = np.empty(N.size)
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq._approx_Z_mean(xo)
.. code:: python

    plot_0d(calc, approx)
    print_0d(calc, approx)

.. parsed-literal::

    error : 1.82270865068e-13
    approx: 0.120285623577
    calc  : 0.120285623577



.. image:: visual-tests_files/visual-tests_37_1.png


Variance of Z
-------------

Test :math:`\mathrm{Var}(Z)`.

.. code:: python

    calc = bq._exact_Z_var()
    approx = np.empty(N.size)
    for i, n in enumerate(N):
        xo = np.linspace(xmin, xmax, n)
        approx[i] = bq._approx_Z_var(xo)
.. code:: python

    plot_0d(calc, approx)
    print_0d(calc, approx)

.. parsed-literal::

    error : 5.20205230419e-11
    approx: 6.05178761461e-07
    calc  : 6.05126740938e-07



.. image:: visual-tests_files/visual-tests_40_1.png

